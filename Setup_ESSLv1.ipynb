{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Setup_ESSL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lyoChxwST7Wc0b5kAFqvwu180PTUl9lj",
      "authorship_tag": "ABX9TyNsWKFhvszxrjqlaOLewpjo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmaKa3XLsn1g",
        "outputId": "254d6d84-9d5c-4e9b-8d65-5631117aaf1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ESSL-WandB' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "############# Setup ESSL in Colab\n",
        "# only do this once! ...or your current work in ESSL folder will be overwritten\n",
        "# if no-file error, first mount gdrive manually \n",
        "\n",
        "!git clone https://github.com/Hackathorn/ESSL-WandB/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install some extra modules\n",
        "!pip install umap-learn\n",
        "# !pip install hdbscan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Jkt3lNWRq_",
        "outputId": "f6fbaff0-c280-4369-d33a-702481d94ac3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.5.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.62.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> TODO add comments\n",
        "'''\n",
        "# %%\n",
        "# import modules\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from utils.experiment import create_experiment, \\\n",
        "                                    get_dict_from_arg, \\\n",
        "                                    save_dataframe, \\\n",
        "                                    load_dataframe\n",
        "\n",
        "from utils.sample import  create_samples\n",
        "\n",
        "from utils.point import create_points, \\\n",
        "                        add_posLowD_to_points, \\\n",
        "                        plot_pos2D_points\n",
        "\n",
        "from utils.edge import    create_edges_with_UMAP, \\\n",
        "                    add_pt_degreeweights_to_points, \\\n",
        "                    add_Euclidean_distance_to_edges\n",
        "                    \n",
        "# from points import load_previous_points\n",
        "# from STEP5_Subgraphs import find_subgraph_cover_from_cliques\n",
        "from utils.subgraph import create_covers_from_edges, plot_covers, create_subgraph_from_covers\n",
        "# from utils.mapping import create_mappings\n",
        "# from STEP6_Mappings import find_peak_step_in_cover \n",
        "# from STEP6_Mappings import plot_density_of_sg_class_map\n"
      ],
      "metadata": {
        "id": "21SRzdd9wGSR",
        "outputId": "84d9d417-8c6c-429a-807a-79aa11a5e005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4848c09c102e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# import matplotlib.pyplot as plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_experiment\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mget_dict_from_arg\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0msave_dataframe\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mload_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0mcreate_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def set_params():\n",
        "    \"\"\"set_params [summary]\n",
        "\n",
        "    Returns:\n",
        "        dict: Key/value for each parameter for process_run. If value is list,\n",
        "                param is a hyper-parameter, for which each combination generates a run\n",
        "    \"\"\"\n",
        "    \n",
        "    dsn = 'MNIST'          # ['MNIST','FASHION\",'DCAI','CHECKER']   # MNIST and/or DCAI \n",
        "    \n",
        "    tag = 'COMBO_base' if isinstance(dsn, list) else dsn + '_base'\n",
        "        \n",
        "    params = {  \n",
        "            ########### parameters for experiment.py - scalar values only\n",
        "            'exper_tag':        # experiment name\n",
        "                tag,\n",
        "            'verbose': \n",
        "                1, \n",
        "            'filelog':          # print log to file\n",
        "                1,\n",
        "            ########### parameters for process_run\n",
        "            #   Use 3-char abrevation as dict key. Key values can be scalar or list.\n",
        "            #   If list, it is a hyper-parameter, which appears in Run folder name.\n",
        "            #   If list len > 1, every combination results in an unique run.\n",
        "            \n",
        "            'dsn':              # dataset_path= MNIST, DCAI, CHECKER >>> TODO path to structured dataset folder\n",
        "                dsn,\n",
        "            'pts':              # points_path= ''; if not null, use path for points_df to bypass VAE training\n",
        "                '', \n",
        "            'img':              # img_size= 32, 64, 96, 128; pixel width/height of square image\n",
        "                32,\n",
        "            'ncl':              # no of classes in samples (CHECKER)\n",
        "                [10],\n",
        "            'nbk':              # no of pixel blocks in image to vary info density (CHECKER)\n",
        "                [1],\n",
        "            'lr':               # learning rate during training\n",
        "                [0.0005],\n",
        "            'dim':              # latent_dim = 8, 16, 32, 64, or any value; dim = 2 & 3 auto perform\n",
        "                # [8,16],\n",
        "                [16],\n",
        "            'epo':              # epoches= 10, 100, 200; no of epoch training cycles\n",
        "                5, \n",
        "            'n_n':              # nearest_neighbor= 2 to 25% of sample size; UMAP  param 15 default\n",
        "                # [2,15,100],\n",
        "                15,\n",
        "            'dis':              # min_dist= 0.0 to 0.99; fine clumpiest to global structure - 0.1 default\n",
        "                # [0.1,0.99],\n",
        "                0.1,\n",
        "            'cut':              # wgt_cutoff= 0.01; ignore edges with weights below cutoff \n",
        "                0.01,\n",
        "        }\n",
        "    \n",
        "    nRuns = np.prod([len(v) for v in params.values() if isinstance(v, list) ])\n",
        "    hparams_keys = [k for k, v in params.items() if isinstance(v, list) ]\n",
        "    if nRuns > 1: print(f'No of Runs = {nRuns} for hparams key = {hparams_keys}')\n",
        "    \n",
        "    return params \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Nk4tt5rqxDny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def process_run(run_folder, params):\n",
        "    \"\"\"process_run routine is executed by create_experiment for each run\n",
        "\n",
        "        Args:\n",
        "        run_folder (str):   path to run folder\n",
        "        params (dict):      See set_params method for param key definitions\n",
        "\n",
        "        Returns:\n",
        "        [dict]: key/value for metrics from this run\n",
        "        \"\"\"\n",
        "\n",
        "    ##### set param variables from params arg\n",
        "    global verbose\n",
        "    verbose = params['verbose'] if 'verbose' in params else True    # >>> TODO test verbose from ESSL.py\n",
        "\n",
        "    ##### set hparam variables from param arg\n",
        "    dataset_path = params['dsn'] if 'dsn' in params else None\n",
        "    # preprocess = params['pre'] if 'pre' in params else True  >>> TODO remove! samples_df is responsible\n",
        "    n_classes = params['ncl'] if 'ncl' in params else 10\n",
        "    n_blk_size = params['nbk'] if 'nbk' in params else 2\n",
        "    points_path = params['pts'] if 'pts' in params else ''\n",
        "    img_size = params['img'] if 'img' in params else 32\n",
        "    nEpochs = params['epo'] if 'epo' in params else 10\n",
        "    latent_dim = params['dim'] if 'dim' in params else 16\n",
        "    n_neighbors = params['n_n'] if 'n_n' in params else 5\n",
        "    min_dist = params['dis'] if 'dis' in params else 0.1\n",
        "    wgt_cutoff = params['cut'] if 'cut' in params else 0 # accept ALL as default\n",
        "    pct_node_covered = params['pct_node_covered'] if 'pct_node_covered' in params else 0.9\n",
        "    \n",
        "    ##### STEP1 - Create samples by preprocessing initial dataset\n",
        "    samples_df = create_samples(dataset_path, img_size, n_classes=n_classes, \n",
        "                                n_blk_size=n_blk_size, verbose=verbose)\n",
        "    # sample_df = convert_ds_to_df(dataset_folder, img_invert=True)\n",
        "    # print('preprocess_images_to_array img_size = ', img_size)\n",
        "    # # sample_df = preprocess_images_to_array(sample_df, img_size)\n",
        "    # sample_df = preprocess_images_to_array(sample_df, 256)      # save as many pixels as possible TODO flags!!!\n",
        "    save_dataframe(run_folder, 'samples_df', samples_df)\n",
        "\n",
        "    ##### STEP2 - Create points by embedding samples into D-dim Latent Space\n",
        "    if points_path == '':   # if no previous points_df, create a new points_df\n",
        "        # points_df = create_points(samples_df, img_size, preprocess, latent_dim, nEpochs, run_folder) >>> TODO remove! samples_df is responsible\n",
        "        points_df = create_points(samples_df, img_size, latent_dim, nEpochs, run_folder, verbose=verbose)\n",
        "        save_dataframe(run_folder, 'points_df', points_df)\n",
        "    else:\n",
        "        points_df = load_dataframe(points_path, 'points_df')\n",
        "\n",
        "    points_df = add_posLowD_to_points(points_df, n_neighbors, min_dist)\n",
        "    save_dataframe(run_folder, 'points_df', points_df)\n",
        "    plot_pos2D_points(points_df, n_neighbors, min_dist, run_folder)\n",
        "\n",
        "    ##### STEP3 -  Create edges by calculating UMAP weights among points >>>TODO add more distance metrics\n",
        "    edges_df, _ = create_edges_with_UMAP(points_df, n_neighbors, min_dist, latent_dim, wgt_cutoff, verbose)\n",
        "    edges_df = add_Euclidean_distance_to_edges(edges_df, points_df, verbose) \n",
        "    # edges_df = add_labels_to_edges(edges_df, points_df)   # TODO FLAG edge if different labels?\n",
        "    save_dataframe(run_folder, 'edges_df', edges_df)\n",
        "\n",
        "    # points_df = add_pt_degreeweights_to_points(points_df, edges_df, run_folder, verbose)\n",
        "    # save_dataframe(run_folder, 'points_df', points_df)\n",
        "    \n",
        "    ##### STEP4 - Create Subgraphs from Edges\n",
        "    # create covers by merging edges, strongest first\n",
        "    covers_df = create_covers_from_edges(edges_df, verbose)\n",
        "    # save_dataframe(run_folder, 'covers_df', covers_df)    # >>> TODO HUGE!!! need to save?\n",
        "    # plot results of cover builds\n",
        "    plot_covers(covers_df, run_folder)\n",
        "    # create the subgraph based on % of points/nodes covered\n",
        "    subgraphs_df = create_subgraph_from_covers(covers_df, run_folder, [70,50,40,20,10])\n",
        "    save_dataframe(run_folder, 'subgraphs_df', subgraphs_df)    \n",
        "\n",
        "    # ##### STEP4 - Create cliques among points with strong edges\n",
        "    # cliques_df = find_cliques_from_edges(points_df, edges_df, umap_object, wgt_cutoff)\n",
        "    # plot_cliq_weight_by_K(cliques_df, wgt_cutoff, run_folder)\n",
        "    # save_dataframe(run_folder, 'cliques_df', cliques_df)\n",
        "\n",
        "    # #### STEP4 - Create subgraphs by merging edges in Graph Space\n",
        "    # covers_df = find_subgraph_cover_from_cliques(cliques_df, wgt_cutoff, run_folder)\n",
        "    # save_dataframe(run_folder, 'covers_df', covers_df)\n",
        "    # subgraphs_df = create_subgraphs(edges_df, pct_node_covered, run_folder)\n",
        "    # save_dataframe(run_folder, 'subgraphs_df', subgraphs_df)\n",
        "\n",
        "    ##### STEP5 - Create mappings of subgraphs to pre/post class labels\n",
        "    # mappings_df = create_mappings(subgraphs_df, points_df, run_folder)   # >>>>>>>> TODO\n",
        "    # save_dataframe(run_folder, 'mappings_df', mappings_df)\n",
        "\n",
        "\n",
        "    ##### Create CSV/JSON for Nodes/Cliqeus to ESSL Workshop    >>>>>> TODO \n",
        "    # save_nodes_for_run(run_folder, points_df, node_lim=None)\n",
        "    # save_cliques_for_run(run_folder, cliques_df, cliq_lim = None, k_lim=None)\n",
        "\n",
        "    # save_nodes_cliques_as_CSV(run_folder, points_df, cliques_df)\n",
        "    # save_nodes_as_json(run_folder, points_df, node_lim=None)\n",
        "    # save_cliques_as_json(run_folder, points_df, Kcliq_lim=None)\n",
        "\n",
        "    ####################### Update results from this runs using metrics from sample_df, etc\n",
        "    ##### metrics from sample_df\n",
        "    m_flagged_images = sum([1 for s in samples_df['flags'] if s != ''])\n",
        "\n",
        "    ##### metrics from points_df\n",
        "    m_flagged_points = sum([1 for s in points_df['flags'] if s != ''])\n",
        "    \n",
        "    mse = np.vstack(points_df.pt_mse_loss).flatten()\n",
        "    m_mse_loss = mse.mean()\n",
        "    z_mse = (mse - mse.mean()) / mse.std()\n",
        "    mse_out = mse[z_mse > 3]\n",
        "    m_mse_out_pct = 100 * len(mse_out) / len(mse)\n",
        "    \n",
        "    m_pos_std = np.vstack(points_df.pt_std).mean()\n",
        "    # m_degree = points_df['pt_degrees'].mean()\n",
        "    # m_weightsum = points_df['pt_weightsum'].mean()\n",
        "\n",
        "    ##### metrics from edges_df\n",
        "    # # m_flagged_edges = sum([1 for s in edges_df['flags'] if s != ''])  >>>>>>>>>>> TODO add 'flags' column\n",
        "    # m_edge_weight = edges_df['weight'].mean()\n",
        "    # m_pct_edge_weight_one = 100 * sum([1 for w in edges_df['weight'] if w == 1.0]) / len(edges_df.index)\n",
        "\n",
        "    ##### metrics from cliques_df\n",
        "    # m_flagged_cliques = sum([1 for s in cliques_df['flags'] if s != ''])  >>>>>>>>>>> TODO add 'flags' column\n",
        "    # m_cliq_weight = cliques_df['weight'].mean()\n",
        "    # m_cliq_ksize = cliques_df['ksize'].mean()          # >>>>>>>>>>>>>> TODO ignore 1-cliques ???\n",
        "    # m_cliq_kmax = cliques_df['ksize'].max()   \n",
        "    # # m_cliq_size = mean[s for s in cliques_df['ksize'] if s > 1])\n",
        "    # m_pct_cliq_weight_one = 100 * sum([1 for w in cliques_df['weight'] if w == 1.0]) / len(cliques_df.index)\n",
        "\n",
        "    ##### metrics from cover_df\n",
        "    # m_flagged_covers = sum([1 for s in covers_df['flags'] if s != ''])  >>>>>>>>>>> TODO add 'flags' column\n",
        "    # m_max_sg_cnt, m_max_sg_step = find_peak_step_in_cover(covers_df)\n",
        "\n",
        "    run_metrics = { 'm_flagged_images': m_flagged_images, \n",
        "                    'm_flagged_points': m_flagged_points,\n",
        "                    'm_mse_loss': m_mse_loss,\n",
        "                    'm_mse_out_pct': m_mse_out_pct,\n",
        "                    'm_pos_std': m_pos_std,\n",
        "                    # 'm_degree': m_degree,\n",
        "                    # 'm_weightsum': m_weightsum,\n",
        "                    # 'm_edge_weight': m_edge_weight, \n",
        "                    # 'm_pct_edge_weight_one': m_pct_edge_weight_one,\n",
        "                    # 'm_cliq_weight': m_cliq_weight, \n",
        "                    # 'm_cliq_ksize': m_cliq_ksize,\n",
        "                    # 'm_cliq_kmax': m_cliq_kmax,\n",
        "                    # 'm_pct_cliq_weight_one': m_pct_cliq_weight_one,\n",
        "                    # 'm_max_sg': m_max_sg_cnt,\n",
        "                    # 'm_max_sg_step': m_max_sg_step,\n",
        "                }\n",
        "\n",
        "    return run_metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "PD5MM2bsJ9zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = set_params()\n",
        "experiment_path = create_experiment(process_run, params)\n"
      ],
      "metadata": {
        "id": "EpUd87wXKa9H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}